{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Text Data and Naive Bayes in SK-Learn (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Give a simple dataset\n",
    "simple_train = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.Learn the 'vocabulary' of the training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.Transform traning data into a sparse matrix\n",
    "simple_train_dtm = vec.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# 4.Print the sparse matrix\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.Convert the sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.Examine the vocabulary and document-term matrix together\n",
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.transform testing data into a document-term matrix \n",
    "simple_test = [\"please don't call me\"]\n",
    "simple_test_dtm = vec.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.Examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.read tab-separated file “sms.tsv”\n",
    "import pandas as pd\n",
    "sms = pd.read_csv('sms.tsv', sep='\\t',header=None, names=['label', 'message'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.convert label to a numeric variable\n",
    "sms['label'] = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.define X and y\n",
    "X = sms.message\n",
    "y = sms.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# 12.split into training and testing sets; and print the shape of training set and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.instantiate the vectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14.learn training data vocabulary, then create document-term matrix\n",
    "vec.fit(X_train)\n",
    "X_train_dtm = vec.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15.transform testing data into a document-term matrix\n",
    "X_test_dtm = vec.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16. train a NaiveBayes model by \"X_train_dtm\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9885139985642498\n"
     ]
    }
   ],
   "source": [
    "# 17.calculate accuracy of predictions\n",
    "predict = nb.predict(X_test_dtm)\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1203    5]\n",
      " [  11  174]]\n"
     ]
    }
   ],
   "source": [
    "# 18.give the confusion matrix\n",
    "print (metrics.confusion_matrix(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574               Waiting for your call.\n",
       "3375             Also andros ice etc etc\n",
       "45      No calls..messages..missed calls\n",
       "3415             No pic. Please re-send.\n",
       "1988    No calls..messages..missed calls\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 19.print message text for the false positives\n",
    "X_test[y_test < predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132    LookAtMe!: Thanks for your purchase of a video...\n",
       "5       FreeMsg Hey there darling it's been 3 week's n...\n",
       "3530    Xmas & New Years Eve tickets are now on sale f...\n",
       "684     Hi I'm sue. I am 20 years old and work as a la...\n",
       "1875    Would you like to see my XXX pics they are so ...\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "4298    thesmszone.com lets you send free anonymous an...\n",
       "4949    Hi this is Amy, we will be sending you a free ...\n",
       "2821    INTERFLORA - It's not too late to order Inter...\n",
       "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "4514    Money i have won wining number 946 wot do i do...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20.print message text for the false negatives\n",
    "X_test[y_test > predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "1  -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "2   0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "3  -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "4   0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "5  -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "6   0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "7  -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "8   0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "9   0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "10  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "11 -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "12  0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "\n",
       "          7         8         9         10        11        12  \n",
       "0  -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  \n",
       "1   0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
       "2  -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
       "3  -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
       "4  -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
       "5   0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
       "6  -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
       "7   1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
       "8  -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
       "9  -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
       "10 -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
       "11  0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
       "12 -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.show what features are correlated with each other\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "y = boston.target\n",
    "d = pd.DataFrame(boston.data)\n",
    "\n",
    "boston = d.corr()\n",
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "ZN      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "INDUS    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "CHAS    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "NOX      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "RM      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "AGE      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "DIS     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "RAD      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "TAX      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "PTRATIO  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "B       -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "LSTAT    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "CRIM    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  \n",
       "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
       "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
       "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
       "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
       "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
       "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
       "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
       "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
       "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
       "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
       "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
       "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.label the columns\n",
    "boston = load_boston()\n",
    "y = boston.target\n",
    "d = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "boston = d.corr()\n",
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3.plot correlations by color\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(boston, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(boston, mask=mask, cmap=\"rainbow\", vmax=.9, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most correlated features' indexes are 9 and 8\n",
      "the most uncorrelated features' indexes are 7 and 4\n"
     ]
    }
   ],
   "source": [
    "# 4.what features are highly correlated?  which features are highly uncorrelated?\n",
    "\n",
    "boston = pd.DataFrame(load_boston().data).corr()\n",
    "\n",
    "for i in np.arange(13):\n",
    "    for j in np.arange(13):\n",
    "        if i<=j:\n",
    "            boston.loc[i][j]=None\n",
    "\n",
    "max =0\n",
    "for i in boston.max():\n",
    "    if i>max:\n",
    "        max = i\n",
    "min =0\n",
    "for i in boston.min():\n",
    "    if i<min:\n",
    "        min = i\n",
    "\n",
    "for i in np.arange(13):\n",
    "    for j in np.arange(13):\n",
    "        if i>j and boston.loc[i][j]==max:\n",
    "            print(\"the most correlated features' indexes are\",i,\"and\",j)\n",
    "for i in np.arange(13):\n",
    "    for j in np.arange(13):\n",
    "        if i>j and boston.loc[i][j]==min:\n",
    "            print(\"the most uncorrelated features' indexes are\",i,\"and\",j)\n",
    "            \n",
    "# corelated features(8,9) are (RAD, TEX)\n",
    "# uncorrelated features(4,7) are (NOX, DIS)\n",
    "# As we can see, in the heatmap RAD&TEX is the most red one and NOX&DIS is the most blue one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.2 23.9 16.6 22.  20.8 23.  27.9 14.5 21.5 22.6 23.7 31.2 19.3 19.4\n",
      " 19.4 27.9 13.9 50.  24.1 14.6 16.2 15.6 23.8 25.  23.5  8.3 13.5 17.5\n",
      " 43.1 11.5 24.1 18.5 50.  12.6 19.8 24.5 14.9 36.2 11.9 19.1 22.6 20.7\n",
      " 30.1 13.3 14.6  8.4 50.  12.7 25.  18.6 29.8 22.2 28.7 23.8  8.1 22.2\n",
      "  6.3 22.1 17.5 48.3 16.7 26.6  8.5 14.5 23.7 37.2 41.7 16.5 21.7 22.7\n",
      " 23.  10.5 21.9 21.  20.4 21.8 50.  22.  23.3 37.3 18.  19.2 34.9 13.4\n",
      " 22.9 22.5 13.  24.6 18.3 18.1 23.9 50.  13.6 22.9 10.9 18.9 22.4 22.9\n",
      " 44.8 21.7 10.2 15.4 25.3 23.3  7.2 21.2 11.7 27.  29.6 26.5 43.5 23.6\n",
      " 11.  33.4 36.  36.4 19.  20.2 34.9 50.  19.3 14.9 26.6 19.9 24.8 21.2\n",
      " 23.9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.train data in linear regression\n",
    "X= load_boston().data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(y_test)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32.37816533 27.95684437 18.07213891 21.63166556 18.93029508 19.96277202\n",
      " 32.2834674  18.06715668 24.72989076 26.85359369 27.23326816 28.57021239\n",
      " 21.18778302 26.94393815 23.37892579 20.89176865 17.11746934 37.73997945\n",
      " 30.51980066  8.44489436 20.86557977 16.21989418 25.13605925 24.77658813\n",
      " 31.40497629 11.02741407 13.82097563 16.80208261 35.94637198 14.7155729\n",
      " 21.23939821 14.15079469 42.72492585 17.83887162 21.84610225 20.40178099\n",
      " 17.50287927 27.00093206  9.80760408 20.00288662 24.27066782 21.06719021\n",
      " 29.47089776 16.48482565 19.38852695 14.54778282 39.39838319 18.09810655\n",
      " 26.22164983 20.60676525 25.09994066 24.48366723 25.02297948 26.84986898\n",
      "  5.01517985 24.12809513 10.72843392 26.83178157 16.8023533  35.48142073\n",
      " 19.50937911 27.43260347 16.58016763 19.151488   10.9990262  32.05016535\n",
      " 36.32672849 21.8596379  24.8158357  25.32934192 23.36795453  6.98356201\n",
      " 16.83774771 20.27043864 20.74890857 21.85918305 34.17775836 27.94673486\n",
      " 24.86029952 34.43415796 18.61651831 24.02302532 34.45439496 13.32264718\n",
      " 20.7154011  30.1583435  17.06611728 24.20119805 19.18051951 16.98160423\n",
      " 26.8073424  41.02666829 14.44767989 23.26993252 14.93803206 21.93017824\n",
      " 22.81878103 29.16467031 36.7033389  20.41387117 17.86800518 17.49942601\n",
      " 25.07246443 21.9827349   8.28652561 21.52177032 16.50788716 33.00114509\n",
      " 24.49693379 25.08491201 38.29621948 28.93273167 14.85478187 34.7429184\n",
      " 35.50029467 32.89599805 20.98069467 16.67849644 34.24728954 39.01179205\n",
      " 21.57169864 15.71337489 27.33121768 18.73350137 27.27438226 21.16402252\n",
      " 26.00459084]\n"
     ]
    }
   ],
   "source": [
    "# 6.predict new values\n",
    "y_model= model.predict(X_test)\n",
    "print(y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.114429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.057130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>2.428546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-21.232624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>2.877234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.006911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-1.471583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.305784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.010675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.996138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.006277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.557414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef\n",
       "CRIM     -0.114429\n",
       "ZN        0.057130\n",
       "INDUS     0.038300\n",
       "CHAS      2.428546\n",
       "NOX     -21.232624\n",
       "RM        2.877234\n",
       "AGE       0.006911\n",
       "DIS      -1.471583\n",
       "RAD       0.305784\n",
       "TAX      -0.010675\n",
       "PTRATIO  -0.996138\n",
       "B         0.006277\n",
       "LSTAT    -0.557414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. negative & positive coefficient\n",
    "boston = pd.DataFrame(load_boston().data, columns=load_boston().feature_names)\n",
    "df_bos = pd.DataFrame(model.coef_, boston.columns, columns=['coef'])\n",
    "df_bos\n",
    "\n",
    "# Negative coefficient indicate that X decreased as y increased. \n",
    "# Which means in this column training data decreased as training target. \n",
    "# Vice versa.\n",
    "# For instance: -0.114429 in CRIM means that for every 0.114429 increase there's a unit decrease in the preditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:  0.7789410172622865\n",
      "Mean squared error:  21.897765396049422\n",
      "Mean absolute error: 3.5748681261275412\n"
     ]
    }
   ],
   "source": [
    "# 8.score the model with 3 matrics\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R squared: \",r2_score(y_test, y_model) )\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean squared error: \",mean_squared_error(y_test, y_model))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean absolute error:\",mean_absolute_error(y_test, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEach residual contributes proportionally to the total amount of error, \\nmeaning that larger errors will contribute linearly to the overall error. \\nA small MAE suggests the model is great at prediction, \\nwhile a large MAE suggests that your model may have trouble in certain areas. \\nA MAE of 0 means that your model is a perfect predictor of the ouputs.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a.1.What do these matrics mean?\n",
    "# R Square:\n",
    "'''\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "It is also known as the coefficient of determination, or the coefficient of multiple determination \n",
    "for multiple regression.\n",
    "\n",
    "The definition of R-squared is the percentage of the response variable variation that is explained by a linear model. \n",
    "'''\n",
    "# Mean Squared Error:\n",
    "'''\n",
    "Mean Squared Error (MSE): it measures the average of the squares of the errors—\n",
    "that is, the average squared difference between the estimated values and what is estimated.\n",
    "'''\n",
    "# Mean Absolute Error:\n",
    "'''\n",
    "Mean Absolute Error (MAE): MAE measures the average magnitude of the errors in a set of predictions, \n",
    "without considering their direction. It’s the average over the test sample of the absolute differences \n",
    "between prediction and actual observation where all individual differences have equal weight.\n",
    "'''\n",
    "# a.2.What are the numbers telling you?\n",
    "# R Square:\n",
    "'''\n",
    "The R² is always going to be between -∞ and 1.\n",
    "When R² is negative it means that the model is worse than predicting the mean.\n",
    "R² is the ratio between how good our model is / how good is the naive mean model.\n",
    "\n",
    "So, '0.7789410172622865' means that the model is better than predicting the mean. \n",
    "'''\n",
    "# Mean Squared Error:\n",
    "'''\n",
    "MSE basically measures average squared error of our predictions. \n",
    "For each point, it calculates square difference between the predictions and the target and then average those values.\n",
    "The higher this value, the worse the model is.\n",
    "It is never negative, since we’re squaring the individual prediction-wise errors before summing them,\n",
    "but would be zero for a perfect model.\n",
    "'''\n",
    "# Mean Absolute Error:\n",
    "'''\n",
    "Each residual contributes proportionally to the total amount of error, \n",
    "meaning that larger errors will contribute linearly to the overall error. \n",
    "A small MAE suggests the model is great at prediction, \n",
    "while a large MAE suggests that your model may have trouble in certain areas. \n",
    "A MAE of 0 means that your model is a perfect predictor of the ouputs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:  0.9979682565279447\n",
      "Mean squared error:  0.17151888213409192\n",
      "Mean absolute error: 0.15217030457770375\n"
     ]
    }
   ],
   "source": [
    "# b.How to improve this model? -- using polynomialRegression\n",
    "x= load_boston().data\n",
    "y= load_boston().target\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))\n",
    "\n",
    "\n",
    "model =  PolynomialRegression(3) # the higher the degree is, the better the model gets.\n",
    "model.fit(x,y)\n",
    "y_model = model.predict(x)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R squared: \",r2_score(y, y_model) )\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean squared error: \",mean_squared_error(y, y_model))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean absolute error:\",mean_absolute_error(y, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
